---
title: "Class08_Mini_Project"
author: "Qihao Liu"
format: pdf
toc: TRUE
---
# Exploratory data analysis

## Data import
Data was downloaded from class website as a .csv file. The csv file is imported into R as a data frame. 

Note that the first column here wisc.df$diagnosis is a pathologist provided expert diagnosis. We will not be using this for our unsupervised analysis as it is essentially the “answer” to the question which cell samples are malignant or benign.
```{r}
# Save your input data file into your Project directory
fna.data <- read.csv("WisconsinCancer.csv")

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)

# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
head(wisc.data)
```
Setup a separate new vector called diagnosis that contains the data from the diagnosis column of the original dataset. We will store this as a factor (useful for plotting) and use this later to check our results. (we want to test clustering to see if it partitions the data correctly, having this would be cheating)

```{r}
diagnosis <- factor(wisc.df$diagnosis) 
head(diagnosis)
```

## Exploratory data analysis
>Q1. How many observations are in this dataset?

There are `r nrow(wisc.df)` observations in this datase

>Q2. How many of the observations have a malignant diagnosis?

There are 212 malignant diagnosis

```{r, error=TRUE}
table(wisc.df$diagnosis)
```
>Q3. How many variables/features in the data are suffixed with _mean?

There are 10 variables/features in the data are suffixed with "_mean"
```{r}
colnames(wisc.df)
grep("_mean",colnames(wisc.df))
length(grep("_mean",colnames(wisc.df)))
```

# Principal Component Analysis

## Performing PCA
Note: use the `prcomp()`function to do PCA

- If we look at the argument in `prcomp()`, we can see that `scale. = FALSE` by default. However, if a column has a high value of variance, and `scale. = FALSE`, PCA will be dominated by columns with large variance (*not a feature, just the nature of numbers for certain parameter, i.e.speed of walking for different people is give or take ~4-5m/s, but the number of hairs is ~90k-150k. Number of hair would have a much larger variance, which will dominate PCA, but it doesn't necessarily makes it a more meaningful way to cluster different groups of people*). That's why **in general we should use `scale. = TRUE`** 

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like you’ve done before.
```{r}

# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

We can see that there's large difference between the standard deviations of different variables, so we should use scaling

Next, we do our PCA
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data,scale. = TRUE)

# Look at summary of results
summary(wisc.pr)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27% of the original variance is captured by PC1

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required, because the Cumulative Proportion for PC3 is 72.636%

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required

## Interpreting PCA results

Now you will use some visualizations to better understand your PCA model.

First, we use gg plot to make scatter plot 

```{r}
library(ggplot2)
head(wisc.pr$x)
ggplot(wisc.pr$x) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```
This plot clearly shows two distinct population of patients within our data set. We are going to compare this plot with some other plots.

A common visualization for PCA results is the so-called biplot.

Create a biplot of the wisc.pr using the biplot() function.

```{r}
biplot(wisc.pr)
```
>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

No thing really stands out about this plot. It is very messy and difficult to understand because the text labels are all clumpped together

> Q8 Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

I noticed that from these scatter plot that the there are clearly two distinct patient population with in the dataset, and these groups can be separated by the diagnosis. Compare PC1 vs PC2 and PC1 vs PC3, we can see that PC1 vs PC2 gives a clearer separation (makes sense because PC1 and PC2 collectively captures more variance)

```{r}
library(ggplot2)
head(wisc.pr$x)
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col = diagnosis) +
  geom_point()


```

## Variance Explained

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2). Save the result as an object called pr.var.

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.

```{r, error = TRUE}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA results
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

The loading vector of the first PC for the feature concave.point_mean is `r wisc.pr$rotation["concave.points_mean", 1]`


>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5
```{r}
summary(wisc.pr)
```


# Hierarchical clustering
Just clustering the original data is not very helpful, so...

First scale the wisc.data data and assign the result to data.scaled.

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```
Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.
```{r}
data.dist <- dist(data.scaled)
```
Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.
```{r}
wisc.hclust <- hclust(data.dist,method = "complete")
```

## Results of hierarchical clustering
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

At height = 19, we can get 4 clusters

- Note: But this is not very helpful because it really doesnt relay any insight/information about the dataset to us. So we give up on this approach

```{r}
plot(wisc.hclust)
head(cutree(wisc.hclust, k=4))
table(cutree(wisc.hclust,k=4))
abline(h=19, col="red", lty=2)
```

## Selecting number of clusters

Next we are going to compare the outputs from the hierarchical clustering model to the actual diagnosis.

Recall, we have the diagnosis factor, maybe it will help if we see where the malignent and benign diagnosis are in the h-clustering?
```{r}
table((cutree(wisc.hclust,k=4)), diagnosis)
```

Next, we are gonna explore how different number of clusters affect the ability of the hierarchical clustering to separate the different diagnosis

>Q12. Can you fin d abetter cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

It is difficulty to find a cluster vs diagnoses match that is meaningful. In comparison, it is slightly better when we cluster the data in 4,5, 6, or 7 clusters as in these cases, most malignent diagnosis is clustered under cluster 1 and most benign diagnosis is clustered under cluster 3, but these clustering methods also generates a number of cluster that is meaningless (can be interpreted as another cluster's equivalent), so without prior knowledge of the diagnosis, they would not be meaningful to us.

```{r}
test_cluster <- function(x){
  table((cutree(wisc.hclust,k=x)), diagnosis)
}
test_cluster(2)
test_cluster(3)
test_cluster(5)
test_cluster(6)
test_cluster(7)
test_cluster(8)
test_cluster(9)
test_cluster(10)
```

This confirms that this clustering is useless.

## Using different methods
>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning

the complete linkage method usually gives me my favorite results for the same data.dist dataset because it always give me distinct populations/clustering by using the largest of all pair-wise similarity to determine the clustering. 

# Optional: K-means Clustering
## k-means clustering and comparing results
We will next try create a k-means clustering model on the Wisconsin breat cancer data and compare the results to the acutual diagnosis

First, we will create a k-means model on `wisc.data`

```{r}
wisc.km <- kmeans(data.scaled,centers=2,nstart=3)
table(wisc.km$cluster, diagnosis)
```
>Q14. How well does k-means separate the two diagnoses?How does it compare to your hclust results?

The k-means separate the two diagnoses fairlyl well, with the malignent and benign diagnoses in cluster 1 and 2 respectively. It is does do a better job compare to the hclust we performed earlier.

```{r}
test_cluster(4)
```


#  Combining methods
So let's try combine PCA with h-clustering since clustering of the origianl data is not productive, PCA is promising, so maybe we can cluster from PCA results (clustering in PC space)

## Clustering on PCA results
Since the first 3 PCs can capture 70% of variance, we can try to cluster them first

```{r}
# First we get the frist 3 PCs, pastt it through a dist matrix function, call it dist.pc
dist.pc <- dist(wisc.pr$x[, 1:3])
wisc.pr.hclust <- hclust(dist.pc, method="ward.D2")
plot(wisc.pr.hclust)
abline(h=70, col="red", lty=2)
```
It is good that we can see there's two main clusters. Now we are gonna try to see if this clustering is good by comparing it with the diagnosis results.

To get our clustering membership vector, we cut the tree at a desired height to yeild 2 cluster (with `cuttree()` at either h=70, or k=2)
We can the use `table()` to compare the clustering results with the diagnosis results.
This clustering groups is fairly accurate when compare to expert diagnosis. (For Diagnosis purpose, for malignent, we have 24 false positives out of 203 cases.)
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
```

# Sensitivity & Specificity

>Q15. How well does the newly created model with four clusters separates out the two diagnosis

The newly created model with 2 clusters separates out eh two diagnosis pretty neatly, as most of the malignent diagnosis is in cluster 1 while most of the benign diagnosis is in cluster 2

>Q16. How well do the k-means and hierarchical clustering you created in previous section do in terms of separating the diagnoses? 

The k-means clustering does a decent job separating the diagnoses (not as well as PCA); the hierarchical clustering does not do a goo job separating the diagnosis.

```{r}
table(wisc.km$cluster,diagnosis)
test_cluster(4)
```

Sensitivity refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples. In other words: TP/(TP+FN).

Specificity relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign. In other words: TN/(TN+FN).

Now let's try calculate these numbers from the matrix
```{r}
N <- table(grps, diagnosis)
N
Sensitivity <- N[1,2]/sum(N[1,])
Sensitivity
Specificity <- N[2,1]/sum(N[2,])
Specificity
```

>Q17. Which of your analysis procedure resulted in a clustering model with the best specificity?How about sensitivity?

Both K-means and PCA-hclust combined clustering both results in the highest specificity. The hierarchical clustering with k=2 has the highest sensitivity, but this is because almost all patients are grouped under cluster 1, so it has good ability to correctly detect ill patients who do have the condition. Otherwise, the sensitivities of the k-means method and the combined method are the same.

```{r}
N <- table(grps, diagnosis)
N
Sensitivity <- N[1,2]/sum(N[1,])
Sensitivity
Specificity <- N[2,1]/sum(N[2,])
Specificity

M <- table(wisc.km$cluster, diagnosis)
M
Sensitivity <- N[1,2]/sum(N[1,])
Sensitivity
Specificity <- N[2,1]/sum(N[2,])
Specificity

O <- table(cutree(wisc.hclust,k=2), diagnosis)
O
Sensitivity <- O[1,2]/sum(N[1,])
Sensitivity
Specificity <- O[2,1]/sum(N[2,])
Specificity

```



# Prediction
We can use our PCA model for predicting with new input patient samples
```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
plot(wisc.pr$x[,1:2],col=diagnosis)
cat(paste(levels(diagnosis),"=",palette()[seq_along(levels(diagnosis))],collapse = ","))
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
>Q18. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize patient 2 based on the results since patient 2 is placed in the cluster with malignent diagnosis.
