---
title: "Class07"
author: "Qihao Liu (PID:U08901197)"
format: pdf
---

Today we will explore some fundamental machine learning methods including clustering and dimensionality reduction.

## K-means clustering

To see how this works, Let's first make up some data to cluster where we konw what the answer should be, we can use the `rnorm()` function to help here.

- this gives us n number of data drawn from a normal distribution with pre-determined mean and SD.

```{r}
rnorm(50)
hist(rnorm(50,15,2))

```
Now let's make up a data in which there's clearly two distinct groups
```{r}
x <- c(rnorm(30, mean=-3), rnorm(30,mean=+3))
y <- rev(x) #reverse x

cbind(x,y) #column bind

z <- cbind(x,y)
plot(z)
```
 - Q: What does cbind do?
 
The function for K-means clustering in "base" R is called `kmeans()` (k in kmeans means how many cluster you want)

```{r}
k <- kmeans(z, centers = 2)
k
```

To get the results of the returned list object, we can use the dollar `$` syntax, for example, to get the size of each cluster:

- Note the components listed fro K-means are essentially different types of results

> Q: how many points are in each cluster

```{r}
k$size
```

> Q: What 'component' gives
  - Membership/ Cluster Assignment
  - Cluster Center
  
```{r}
k$cluster #Membership
k$centers #Cluster Center
```

  
> Q: Make a result figure of the data colored by cluster membership and show cluster centers

```{r}
plot(z, col = c("red","darkgreen")) #this colors the dots in alternating colors
plot(z, col = 2) #color by bumber
plot(z, col = k$cluster, pch=20) # this basically tells R, use the number indicating each pt's membership to color them.
points(k$centers, col = "lightblue", cex=2, pch=15)
```

K-means clustering is very popular as it is very fast and relatively straight forward: it takes numeric data as input and returns the cluster membership vector etc.
But the "issue/feature" of it is that you need to tell `kmeans()` how many cluster we want!

>Q: Run Kmeans() again and cluster into 4 groups, and plot the results
So the result is not ideal...

```{r}
k4 <- kmeans(z, centers=4)
k4$cluster
k4$centers
plot(z, col = k4$cluster, pch=20)
points(k$centers, col = "lightblue", cex=2, pch=15)
```

An alternative way is to make a **Scree Plot**, graphing total variation within a cluster (measured as Total Within-Cluster Sum of Squares)against the number of centers, and we can take the "elbow" center (the centers n that has the greatest decrease in total variation within a cluster compare to n-1) to be the optimal one.

- Note scree is the place underneath a cliff that the rubbles collects
```{r}
k1 <- kmeans(z, centers=1)
k2 <- kmeans(z, centers=2)
k3 <- kmeans(z, centers=3)
k4 <- kmeans(z, centers=4)
k4$tot.withinss
k5 <- kmeans(z, centers=5)
k6 <- kmeans(z, centers=6)
k7 <- kmeans(z, centers=7)
x <- c(1,2,3,4,5,6,7)
y <- c(k1$tot.withinss, k2$tot.withinss, k3$tot.withinss, k4$tot.withinss, k5$tot.withinss, k6$tot.withinss, k7$tot.withinss)
plot(x,y, type="l", xlab = "Centers", ylab = "Total Within SS")
text(x[2], y[2], labels = "centers = 2", pos = 3, col = "darkred")
```
or we can use `for()` loop
```{r}
n <- NULL
for(i in 1:5) {
  n <- c(n,kmeans(x, centers = i)$tot.withinss)
  plot(n, typ="b")
}
```

## Hierarchical Clustering

this meain "base" R function for Hierarchical Clustering is called `hclust()`

- the only required input of `hclust()` is d, a disimilarity matrix generated by the `dist()` function

```{r, error=TRUE}
hclust()
```


- `dist()` function calculate a distance matrix for our data, its a analysis of all paralle comparision of every point in data (it gives us a triangle because it is symmetrical)

```{r}
d <- dist(z)
hc <- hclust(d)
hc
plot(hc)
abline(h=10, col = "red")
cutree(hc,h=10)
```

if we look at the dendrogram, we can clearly see that there are two large clusters, and higher numbers are clustered on one side of the plot, and lower numbers are clustered on the other side of the plot (this is related to how we build the data, we get 30 numbers with one mean, and 30 other numbers with a different mean)

Horizontal line contains the important information. We can see that if we cut the dendrogram at the dark red line with the `cutree ()`  function, it would yield a membership output

To get our cluster membership output (i.e. our main clustering results), we can cut the tree at a given height or at a height that yields a diven "k"
```{r}
grps <- cutree(hc,h=10)
#OR
cutree(hc,k=2)
```
>Q: Plot the data with our hclut results coloring

```{r}
plot(z, col= grps)
```

## Dimensionality Reduction: Principle Component Analysis(PCA)
The major goal of PCA is dimensionality reduction and lost as little information/essence of data as possible

2D-Example:
- The first principle component (PC) follows a best fit through the data points. 
- The second principle component picks up the wiggling of the data along the first PC
- Combined, these captures most of the spread of the data and can act as new axis
- The data should have max variation on PC1, PC2 captures the rest of the variants (Generally, for PC1,2,3, the variation captured decreases as the numbering goes up)

# Principal Component Analysis (PCA)
## PCA of UK Food Data

Import food data from an online CSV file.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
head(x)
```


>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

 the for the first column, the name is X, and the row names are not food but numbers
To fix this:
We can try....
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
x
```
This is no good because the code is overwriting itself, so each time we run it, we lose some data.

Instead, we do
```{r}
x <- read.csv(url,row.names=1)
x
```
Now we can try some base figures:
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
>Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
These plot are usless

>Q5: We can use the pairs() function to generate all pairwise plots for our countries. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

There is one plot that can be useful FOR SAMLL DATASET

```{r}
pairs(x, col=rainbow(10), pch=16)
```
To read this plot, if we take the first row where it starts with "England", we can see England vs Wales, England vs Scotland, England vs N. Ireland

- If the country eats exactly the same, it will be an diagnal line
- deviation from the diagnal highligths the differences in eating
- from this we can see that N. Ireland is very different 
>Main point: it can be difficult to spot major trends and patterns even in relatively small multivariate dataset (here we only have 17 dimension, typically it could be 1000s)

## PCA to the rescue
Now let's try PCA on this data to see if it is actually useful. The main functionin "base" R for PCA is called `prcomp()`

- First we transpose the data so food is in the columns. This allow us to do PCA on the food with `t()` transpose function.
```{r}
t(x)
PCA <- prcomp(t(x))
PCA
summary(PCA)
```

Note this summary tells us that PC1 captures 67.44% of the variants, PC2 29.05%, and PC3 3.5%. Together, PC1 and 2 captures >95% of the variance

Now let's try to plot the results in "base" R and ggplot

- First figure out what we are plotting, we are plotting X, which contains the PCs, and since PC1 and PC2 capture most of the variation, we are only gonna plot them two

```{r}
PCA$x
cols <- c("orange","darkgreen","darkblue","darkred")
plot(PCA$x[,1], PCA$x[,2], col=cols,pch=16, xlab = "PC1", ylab = "PC2")
```
If we are gonna use ggplot
>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
library(ggplot2)
ggplot(PCA$x)+
  aes(PC1,PC2)+
  geom_point(col=cols)
```
From this graph, we can clearly see that N. Ireland is distinct from the rest of the countrys from the survey.

But what food contributes to these PCs?
We can figure this out, with `PCA$rotation`, we can see the contribution of each food to the different PCs, now if we plot it we can visualize what contributes to the PCs

```{r}
PCA$rotation
ggplot(PCA$rotation)+
  aes(PC1,rownames(PCA$rotation))+
  geom_col()
```
This graph tells us that Ireland consume more soft drink and fresh potatoes

PCA looks super useful, and we will come back to describe this further next class
